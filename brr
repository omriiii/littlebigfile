import datetime
import os.path
import random
import sys
import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

import time
import os
import math

DEVICE = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)

print(f"Script using {DEVICE} device")



# Print iterations progress
def printProgressBar(iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ', printEnd = "\r"):
    """
    Call in a loop to create terminal progress bar
    @params:
        iteration   - Required  : current iteration (Int)
        total       - Required  : total iterations (Int)
        prefix      - Optional  : prefix string (Str)
        suffix      - Optional  : suffix string (Str)
        decimals    - Optional  : positive number of decimals in percent complete (Int)
        length      - Optional  : character length of bar (Int)
        fill        - Optional  : bar fill character (Str)
        printEnd    - Optional  : end character (e.g. "\r", "\r\n") (Str)
    """
    percent = ("{0:." + str(decimals) + "f}").format(100 * (iteration / float(total)))
    filledLength = int(length * iteration // total)
    bar = fill * filledLength + '-' * (length - filledLength)
    print(f'\r{prefix} |{bar}| {percent}% {suffix}', end = '')
    # Print New Line on Complete
    if iteration == total:
        print()


class FileBytearrayAsDataset(Dataset):
    def __init__(self,
                 file_bytearray: bytearray,
                 incorrectness_mask: bytearray,
                 k_values: list,
                 output_size: int):

        self.file_bytearray = file_bytearray
        self.samples_cnt = len(self.file_bytearray)

        self.incorrectness_mask = incorrectness_mask
        self.output_size = output_size


        #
        # Create static x values
        self.static_xs = []
        for k in k_values:
            if k <= self.output_size:
                self.static_xs.extend([np.sin(2 * x * np.pi / k).astype('float32') for x in range(k)])
                self.static_xs.extend([np.cos(2 * x * np.pi / k).astype('float32') for x in range(k)])
                self.static_xs = list(set(self.static_xs))

        self.static_xs = torch.from_numpy(np.array(self.static_xs, dtype='float32'))
        self.ks = [k for k in k_values if k > self.output_size]

    def __len__(self):
        return self.samples_cnt

    def __getitem__(self, idx):

        #
        # CALCULATE DYNAMIC X
        # TODO: CUSTOM X SAMPLE DISTRIBUTION (INSTEAD OF range(self.output_soze)) HERE!
        x = []
        for k in self.ks:
            x.extend([np.sin((2 * x * np.pi) / k) for x in range(self.output_size)])
            x.extend([np.cos((2 * x * np.pi) / k) for x in range(self.output_size)])
            x = list(set(x))

        ret_x = torch.cat((self.static_xs, torch.from_numpy(np.array(x, dtype='float32'))))

        #
        #   CALCULATE Y
        # This is a condensed call of the commented code below
        y =      [(self.file_bytearray    [math.floor((idx+i)/8)] >> ((idx+i)%8)) & 1 for i in range(self.output_size)]
        y_mask = [(self.incorrectness_mask[math.floor((idx+i)/8)] >> ((idx+i)%8)) & 1 for i in range(self.output_size)]

        ret_y = (torch.from_numpy(np.array([y], dtype='float32')),
                 torch.from_numpy(np.array([y_mask], dtype='float32')))

        return ret_x, ret_y


class NeuralNetwork(nn.Module):
    def __init__(self,
                 input_size: int,
                 output_size: int,
                 layer_size: int = 512,
                 layer_cnt: int = 4):

        super().__init__()
        self.flatten = nn.Flatten()

        if layer_cnt > 0:
            layers = [nn.Linear(input_size, layer_cnt), nn.LeakyReLU()]

            for _ in range(layer_cnt - 1):
                layers.extend([nn.Linear(layer_size, layer_size), nn.LeakyReLU()])

            layers.extend([nn.Linear(layer_size, output_size), nn.Sigmoid()])
        else:
            layers = nn.Sequential(nn.Linear(input_size, output_size),
                                   nn.Sigmoid())

        self.linear_relu_stack = nn.Sequential(*layers)
        """
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(input_size, layer_size),
            nn.ReLU(),
            nn.Linear(layer_size, layer_size),
            nn.ReLU(),
            nn.Linear(layer_size, layer_size),
            nn.ReLU(),
            nn.Linear(layer_size, 2),
            nn.Softmax(dim=-1),
        )
        """

    def forward(self, x):
        logits = self.linear_relu_stack(x)
        return logits


import os
import secrets

def generateRandomBytearray(num_bytes):
    return bytearray(secrets.token_bytes(num_bytes))

def train(file_bytearray: bytearray,
          bits_to_focus_on_mask: bytearray,
          output_size: int,
          k_values: list,
          layer_cnt: int,
          layer_size: int,
          epochs: int,
          learning_rate: float,
          batch_size: int,
          shuffle: bool):

    dataset = FileBytearrayAsDataset(file_bytearray=file_bytearray,
                                     incorrectness_mask=bits_to_focus_on_mask,
                                     k_values=k_values,
                                     output_size=output_size)

    print(f"Training on {len(dataset)} samples")
    print(f"Batch size {batch_size}")

    temp_sample = dataset[0]

    print("Creating data loader...")
    dataloader = DataLoader(dataset=dataset,
                            batch_size=batch_size,
                            shuffle=shuffle)

    print("Creating MLP...")
    mlp = NeuralNetwork(input_size=temp_sample[0].shape[0],
                        output_size=output_size,
                        layer_cnt=layer_cnt,
                        layer_size=layer_size).to(DEVICE)


    optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate)
    loss_fn = nn.MSELoss()

    mlp.train()
    for epoch in range(epochs):
        start_timestamp = time.time()
        correct_labels = torch.tensor(0).to(DEVICE)

        for batch, (batch_x, batch_y) in enumerate(dataloader):
            batch_x = batch_x.to(DEVICE)
            batch_y_truth, batch_y_mask = batch_y
            batch_y_mask = batch_y_mask.to(DEVICE)
            batch_y_truth = batch_y_truth.to(DEVICE)

            pred = mlp(batch_x)

            loss = loss_fn(pred * batch_y_mask, batch_y_truth * batch_y_mask)
            correct_labels += torch.sum(torch.round(pred * batch_y_mask) == (batch_y_truth * batch_y_mask))

            # Backpropagation
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            time_elapsed = time.time() - start_timestamp
            seconds_per_batch = time_elapsed / max(1, batch)
            remaining_batches = math.ceil(len(dataset) / batch_size) - batch
            printProgressBar(iteration=batch,
                             total=math.ceil(len(dataset) / batch_size),
                             length=20,
                             suffix=f" | Time left: {datetime.timedelta(seconds=seconds_per_batch*remaining_batches)}")


        accurecy = correct_labels / len(dataset)

        print(f"{epoch}: accurecy: {accurecy}")
        """
        if batch % 1000 == 0:
            loss = loss.item()
            print(f"[{datetime.datetime.fromtimestamp(time.time())}] loss: {loss:>7f}  [{batch:>5d}/{size:>5d}]")
        """

    torch.save(mlp, "test_mlp")

def getFileByteArray(fname: str) -> bytearray:
    with open(fname, 'rb') as f:
        return f.read()


if __name__ == '__main__':

    # USAGE: python3 main.py filename
    """
    fname = sys.argv[1]
    if not os.path.isfile(fname):
        raise RuntimeError(f"Cannot find file {fname}")
    """

    # lol. override fname
    #fname = '/home/omri/Downloads/Concept Art.zip' # 100mb
    #fname = '/home/omri/Downloads/02 Justice - Sure You Will.wav' # 50mb
    #fname = '/home/omri/Downloads/JUSTICE_RIPOFF_4.mpga' # 1.8mb
    file_bytearray = getFileByteArray("file")


    bits_to_focus_on_mask = [int(random.random() > (2/3)) for _ in range(len(file_bytearray)*8)]

    from itertools import zip_longest  # izip_longest python2
    it = iter(map(str, bits_to_focus_on_mask))
    bits_to_focus_on_mask = bytearray([int("".join(sli), 2) for sli in zip_longest(*iter([it] * 8), fillvalue="")])

    k_values = list(range(2, 10)) + [2048]

    train(file_bytearray=file_bytearray,

          bits_to_focus_on_mask=bits_to_focus_on_mask,
          output_size=1024,
          k_values=k_values,

          layer_cnt=3,
          layer_size=64,
          epochs=500,
          learning_rate=0.00075,
          batch_size=512, # 8192
          shuffle=False)

    """
    Compression Efficeny = Original size of file / (NN file size + 'missed bits table')
    """
